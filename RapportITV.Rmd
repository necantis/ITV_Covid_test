---
title: "Rapport ITV"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r, include=FALSE}
knitr::opts_chunk$set(echo = FALSE , warning = FALSE , message = FALSE)

# The knowledge discovery in databases (KDD) process is commonly defined with the stages:

# 1. Selection
library(xlsx) # Reading participants answers

# 2. Pre-processing
library(tidyverse) # data analysis

library(tidytext)
library(plyr) ## Round_any

# 3. Transformation
library(quanteda) # Text mining
library(tidymodels)

# 4. Data mining

library(stm)
# STM has a problem with RCPP that needs to be solved every time
# https://stackoverflow.com/questions/68416435/rcpp-package-doesnt-include-rcpp-precious-remove
# install.packages('Rcpp')
library(Rcpp) # 

library(infer)

# library(h2o) #Does not work with ShinyApps

# 5. Interpretation/evaluation
# library(tufte) # For the Handout. Source: https://rstudio.github.io/tufte/

library(ggplot2) # Graphs

library(knitr) # # This way to add KableExtra works ...
library(kableExtra) # ... Source: https://stackoverflow.com/questions/49044753/scale-kable-table-to-fit-page-width

library(dotwhisker) # Viz for linear regression
# library(jtools) # Viz for linear regression (does not handle A*B)

# Remove NA when printing results
# Source: https://stackoverflow.com/questions/27626461/hiding-nas-when-printing-a-dataframe-in-knitr
options(knitr.kable.NA = '')

```

## Collected data

```{r data collection, include=FALSE}

#Dealing with French characters
fixAccents <- function (myFrenchText){
  myTemp <- iconv(myFrenchText, from="UTF-8", to="LATIN1")
  myTemp
}

fixAccentsDataframe <- function(myFrenchDataframe){
  myTempDf <- myFrenchDataframe
  for(i in 1:dim(myTempDf)[2]){
    if(typeof(myTempDf[,i])!="integer") # If it's not an integer ...
      myTempDf[,i]<- fixAccents(myTempDf[,i])
  }
  myTempDf
}


# Gettings the answers
answers_raw <- fixAccentsDataframe(
  read.xlsx("data/data.xlsx", header = TRUE, sheetName = "Merged")
)


# Clean data and rename columns
colnames(answers_raw) <-  c(
"ID",	"Heure de début",	"Heure de fin",	
"Adresse de messagerie",	"Nom",	"Langue",	
"Connaissez-vous quelqu'un qui a déjà fait un test COVID ?"	,
"Pensez-vous que les tests COVID en général sont douloureux ?",	
"Quel test réalisez-vous le plus fréquemment ?"	,
"Prendre l'avion"	,
"Se rendre au travail",	
"Se rendre à son lieu de formation"	,
"Voyager dans un autre pays (train, voiture, etc)"	,
"Voir des proches"	,
"Activités spécifiques (boites de nuit, compétitions sportives, etc)"	,
"Evènements (spéctacles, foires, etc)"	,
"Symptômes",
"Racontez-moi la dernière expérience que vous avez eu lorsque vous avez fait un test COVID ?"	,
"Coût"	,
"Facilité d'accès (proximité du domicile)"	,
"Délais entre la prise de rdv/achat et le test"	,
"Simplicité d'utilisation"	,
"Informations claires"	,
"Précision des résultats du test"	,
"Rapidité de la délivrance des résultats"	,
"Sentiment de sécurité"	,
"Certification et validité"	,
"Test PCR (~150 CHF)"	,
"Test antigénique avec certificat (~30 CHF)"	,
"Auto-Test après les 5 gratuits (10 CHF)"	,
"Test PCR (~150 CHF)2"	,
"Test antigénique avec certificat (~30 CHF)2"	,
"Auto-Test après les 5 gratuits (10 CHF)2"	,
"Pour finir ... si vous aviez trois souhaits pour changer les choses lorsque vous vous faites tester, lesquels seraient-ils ?"	,
"Etape 1 : Rendez-vous"	,
"Etape 2 : Test"	,
"Etape 3 : Résultat"	,
"Pour quelle(s) raison(s) pourriez-vous utiliser cette nouvelle solution de test ?"	,
"Imaginez maintenant que vous vous trouvez dans la situation spécifique d'une nouvelle vague de COVID en octobre 2021, quelle(s) méthode(s) voudriez-vous utiliser pour vous faire tester ?"	,
"En faisant l'hypothèse que l'assurance ne paie pas, quel serait selon vous le prix CORRECT  pour cette nouvelle solution de test ?",
"En faisant l'hypothèse que l'assurance ne paie pas, quel serait le prix MAXIMAL que vous seriez prêt(e) à payer pour cette nouvelle solution de test ?"	,
"Très bien. Maintenant faites l'hypothèse que cette solution puisse être offerte à 100CHF/test. Quelle est la probabilité que vous la recommandiez à un ami(e) ou à un de vos proches ?"	,
"Expliquez votre choix :"	,
"Quel est votre âge"	,
"Sexe"	,
"Etes-vous vaccinés?"	,
"Extra: Commentaires"	,
"Autres",
"Prix",
"Connaître les variants",
"Quel est votre niveau de formation ?"
)

```

We have collected `r dim(answers_raw)[1]` answers. 

```{r}
## Convert factor to integer and keep levels
# https://www.datamentor.io/r-programming/subplot/

# par(mfrow=c(1,2))
# barplot(prop.table(table(answers_raw$`Quel est votre âge`)))
# barplot(prop.table(table(answers_raw$Sexe)))
# 
# par(mfrow=c(1,1))
# barplot(prop.table(table(answers_raw$Sexe,answers_raw$`Quel est votre âge`)),beside=TRUE, main="Age distribution across sexes")


# https://www.r-graph-gallery.com/48-grouped-barplot-with-ggplot2.html 
ggplot(
  data.frame(Sexe=answers_raw$Sexe, Age=answers_raw$`Quel est votre âge`)%>%
    group_by(Sexe,Age)%>%
    dplyr::summarise(Answers=n())
       , aes(fill=Sexe, y=Answers, x=Age)) + 
    # geom_bar(position="dodge", stat="identity")
    geom_bar(position="stack", stat="identity")
```



```{r, include=FALSE}
Prix_Correct = as.integer(
    answers_raw$`En faisant l'hypothèse que l'assurance ne paie pas, quel serait selon vous le prix CORRECT  pour cette nouvelle solution de test ?`)

Prix_Max= as.integer(
    answers_raw$`En faisant l'hypothèse que l'assurance ne paie pas, quel serait le prix MAXIMAL que vous seriez prêt(e) à payer pour cette nouvelle solution de test ?`)

Promoter_Score= as.integer(
    answers_raw$`Très bien. Maintenant faites l'hypothèse que cette solution puisse être offerte à 100CHF/test. Quelle est la probabilité que vous la recommandiez à un ami(e) ou à un de vos proches ?`)
```

Collected data shows that most people do not think that they should pay for testing (the fair price for most people is below 50 CHF) . 
But when asked what is the maximum amount they are willing to pay, a new cluster emerges (Graph 02).  
Indeed, some people are willing to pay 100 CHF for the new test, and they are going to recommend it to other users (NPS).  

```{r}
par(mfrow=c(1,2))
hist(Prix_Correct,main="Correct Price", xlim=c(0,250))
hist(Prix_Max,main="Max Price (WTP)", xlim=c(0,250))

par(mfrow=c(1,1))
hist(Promoter_Score,main="NPS for  CHF 100")

```
\newpage

## Looking for Champions

```{r}
myChampions <- answers_raw%>%
  mutate(WTP=as.integer(
           `En faisant l'hypothèse que l'assurance ne paie pas, quel serait le prix MAXIMAL que vous seriez prêt(e) à payer pour cette nouvelle solution de test ?`)
         )%>%
mutate(NPS=as.integer(
         `Très bien. Maintenant faites l'hypothèse que cette solution puisse être offerte à 100CHF/test. Quelle est la probabilité que vous la recommandiez à un ami(e) ou à un de vos proches ?`)
         )


myChampions%>%
  group_by(WTP,NPS)%>%
  dplyr::summarise(n=n())%>%
  ggplot(aes(x=NPS, y=WTP, size = n)) +
              geom_point(alpha=0.7)
```


So how can we predict if a customer is a good customer ? 

We are currently looking for customers 

* that are willing to pay at least 100 CHF and 

* that have an estimated probability to recommend our product of at least 7/10 (every customer with a score below 7 is considered a detractor)

```{r}
myChampionsList <- myChampions%>%
  filter(WTP>=100 & NPS>=7)

myNotChampionsList <- myChampions%>%
  filter(WTP<100 | NPS<7)


myChampionRatio <- round_any(dim(myChampionsList)[1]/dim(myChampions)[1],0.1)
```
After 2 rounds of interviews, we had `r dim(myChampionsList)[1]` champions over a total of `r dim(myChampions)[1]` respondents (=`r myChampionRatio`%).

\newpage


```{r, eval= FALSE}
## Effect of the customer journey on promoter score

# The steps in the customer journey seems to have an effect on the willingness to pay of the customers.   
# Nonetheless, the collected sample was fairly small and we predict what is the probability that the results could change, if new data is collected.   
# The probability that the difference between the NPS scores of those who liked the new "Before" stage (wrt those who did not like it) will change, if new data is collected, is: `r round_any(p_value_2_sample$p_value,.01)` (p-value).

myChampions_infer <- df%>%
  mutate(Before_test = factor(
                      ifelse(
                        Before %in% c("Worse"),
                        "Worse",
                        ifelse(
                            Before %in% c("Same"),
                            NA,
                            "Better"
                            )
                          )
                      )
            )


observed_statistic <- myChampions_infer %>%
  specify(NPS ~ Before_test) %>%
  calculate(stat = "diff in means", order = c("Worse","Better"))

# observed_statistic

# generate the null distribution with randomization
null_dist_2_sample <- myChampions_infer %>%
  specify(NPS ~ Before_test) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("Worse","Better"))


null_dist_2_sample %>%
  visualize() + 
  shade_p_value(observed_statistic,
                direction = "two-sided")

p_value_2_sample <- null_dist_2_sample %>%
  get_p_value(obs_stat = observed_statistic,
              direction = "two-sided")


```


```{r, eval= FALSE}
# The probability that the difference between the NPS scores of those who liked the new "DUring" stage or thought it was the same (wrt those who did not like it) will change, if new data is collected, is: `r round_any(p_value_2_sample$p_value,.01)` (p-value)

myChampions_infer <- df%>%
  mutate(During_test = factor(
                      ifelse(
                        During %in% c("Worse"),
                        "Worse",
                        ifelse(
                            During %in% c("nothing"),
                            NA,
                            "Better"
                            )
                          )
                      )
            )


observed_statistic <- myChampions_infer %>%
  specify(NPS ~ During_test) %>%
  calculate(stat = "diff in means", order = c("Worse","Better"))

# observed_statistic

# generate the null distribution with randomization
null_dist_2_sample <- myChampions_infer %>%
  specify(NPS ~ During_test) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("Worse","Better"))


null_dist_2_sample %>%
  visualize() + 
  shade_p_value(observed_statistic,
                direction = "two-sided")

p_value_2_sample <- null_dist_2_sample %>%
  get_p_value(obs_stat = observed_statistic,
              direction = "two-sided")

```


```{r, eval= FALSE}

# The probability that the difference between the NPS scores of those who liked the new "After" stage (wrt those who did not like it) will change, if new data is collected, is: `r round_any(p_value_2_sample$p_value,.01)` (p-value)


myChampions_infer <- df%>%
  mutate(After_test = factor(
                      ifelse(
                        After %in% c("Worse"),
                        "Worse",
                        ifelse(
                            After %in% c("nothing","Same"),
                            NA,
                            "Better"
                            )
                          )
                      )
            )


observed_statistic <- myChampions_infer %>%
  specify(NPS ~ After_test) %>%
  calculate(stat = "diff in means", order = c("Worse","Better"))

# observed_statistic

# generate the null distribution with randomization
null_dist_2_sample <- myChampions_infer %>%
  specify(NPS ~ After_test) %>%
  hypothesize(null = "independence") %>%
  generate(reps = 1000, type = "permute") %>%
  calculate(stat = "diff in means", order = c("Worse","Better"))


null_dist_2_sample %>%
  visualize() + 
  shade_p_value(observed_statistic,
                direction = "two-sided")

p_value_2_sample <- null_dist_2_sample %>%
  get_p_value(obs_stat = observed_statistic,
              direction = "two-sided")
```


## Effect of the customer journey on the NPS associated with 100 CHF

```{r}
df<- cbind(myChampions[,35:37],
           myChampions[,41:42],
           myChampions[,44:46]
           # myChampions[,49:51] # No NA Values for Random forest
           )%>%  
  dplyr::mutate(Before=factor(`Etape 1 : Rendez-vous`), .keep="unused")%>%
  dplyr::mutate(During=factor(`Etape 2 : Test`), .keep="unused")%>%  
  dplyr::mutate(After=factor(`Etape 3 : Résultat`), .keep="unused")%>%
  dplyr::mutate(WTP=as.integer(
           `En faisant l'hypothèse que l'assurance ne paie pas, quel serait le prix MAXIMAL que vous seriez prêt(e) à payer pour cette nouvelle solution de test ?`), .keep="unused")%>%
    dplyr::mutate(NPS=as.integer(
         `Très bien. Maintenant faites l'hypothèse que cette solution puisse être offerte à 100CHF/test. Quelle est la probabilité que vous la recommandiez à un ami(e) ou à un de vos proches ?`), .keep="unused")%>%
    dplyr::mutate(NPS_Fact=factor(ifelse(NPS<7,"Detractor",ifelse(NPS<9,"Neutral","Supporter"))), .keep="all")%>%
    dplyr::mutate(Age=factor(`Quel est votre âge`), .keep="unused")%>%
    dplyr::mutate(Sex=factor(`Sexe`), .keep="unused")%>%  
    dplyr::mutate(Vaccine=factor(`Etes-vous vaccinés?`), .keep="unused")%>%
  drop_na(NPS)
```

```{r}

myLM<-  linear_reg() %>% 
  set_engine("lm")%>%
  # set_engine("lm")%>% 
  fit(NPS ~ 0 +Age*TestExperience + TestResults, 
      data = df%>%
            select(-c(WTP,NPS_Fact))%>%
            mutate(TestResults = relevel(After, ref = "Équivalent aux tests que j'utilise actuellement"))%>%
            mutate(TestExperience = relevel(During, ref = "Équivalent aux tests que j'utilise actuellement"))%>%
            mutate(Age = relevel(Age, ref = "<20 ans"))
  )

myLM%>%
  tidy()%>%knitr::kable()%>%kable_styling("striped")

  # fit(NPS ~ ., data = df%>%select(-c(WTP,NPS_Fact)))
```

```{r}
myLM_base <- lm(NPS ~ Before+Vaccine+After+During*Age , data = df)
# 

df_simple <- df%>%
  mutate(Before = ifelse(
                        Before %in% c("Plutôt meilleur que les tests que j'utilise actuellement",
                                     "Meilleur que les tests que j'utilise actuellement")
                        , "Better"
                        , ifelse(
                              Before =="Équivalent aux tests que j'utilise actuellement"
                              , "Same"
                              , "Worse"
                              )
                        ), .keep="unused")%>%
  mutate(During = ifelse(
                        During %in% c("Plutôt meilleur que les tests que j'utilise actuellement",
                                     "Meilleur que les tests que j'utilise actuellement")
                        , "Better"
                        , ifelse(
                              During =="Équivalent aux tests que j'utilise actuellement"
                              , "Same"
                              , "Worse"
                              )
                        ), .keep="unused")%>%
  mutate(After = ifelse(
                        After %in% c("Plutôt meilleur que les tests que j'utilise actuellement",
                                     "Meilleur que les tests que j'utilise actuellement")
                        , "Better"
                        , ifelse(
                              After =="Équivalent aux tests que j'utilise actuellement"
                              , "Same"
                              , "Worse"
                              )
                        ), .keep="unused")


### PLOT SUMMS does not work with During*Age
## MOving to dotwhisker

# plot_summs(
#           # myLM_base
# 
#            , scale = TRUE
#            , plot.distributions = TRUE
#            , inner_ci_level = .95
#            )

### DOTWHISKER to explain linear regression

# https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html
  dwplot(list(
          lm(NPS ~ 0+After+During*Age , data = df),
          lm(NPS ~ 0+Vaccine+Before+After+During*Age , data = df)
          ),
         # dot_args = list(size = 2, color = "black"),
         # whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2),
         vars_order = c("Before", "During", "After", "Age", "Vaccine"),
         model_order = c("M1: Simple", "M2: Complete")
         )+
    theme_bw(base_size = 11) + 
    # Setting `base_size` for fit the theme
    # No need to set `base_size` in most usage
    xlab("Coefficient Estimate") + ylab("") +
    geom_vline(xintercept = 0,
               colour = "grey60",
               linetype = 2) +
    ggtitle("Predicting NPS") +
    theme(
        plot.title = element_text(face = "bold"),
        legend.position = c(0.007, 0.01),
        legend.justification = c(0, 0),
        legend.background = element_rect(colour = "grey80"),
        legend.title = element_blank()
    ) 

```

The linear regression analysis shows that respondents _aged >40_ have the tendency to give at least 7/10 for the NPS (p<0.01).  
If the _Test result_ phase is perceived as better than the current one, the scores goes up 3 points (p<0.05).
An loss in quality of _Testing phase_ leads to 1 point less, whereas an improvement in the _Testing phase_ in itself seems to be more problematic to understand: if respondents have less somewhere between 21 and 30 years, there is not effect (-9.5 + 9.35).
Instead, if they are aged between 41 and 50 years old, the score might be going down 7 points (p>0.40).

Although the model has many variables its explanatory power is fairly good: the Adjusted R2 of the model is `r round_any((myLM%>%glance())$adj.r.squared,.01)`. The mode with Vaccine and Before has an Adj R2 of `r round_any((myLM_base%>%glance())$adj.r.squared,.01)`

```{r, eval=FALSE}
### Are these coefficients reliable? Results from Bayesan network analysis
library(rstanarm)

## LM with tidy models; Removing the intercept to smplify the analysis
# https://www.tidymodels.org/learn/statistics/tidy-analysis/
# https://stackoverflow.com/questions/7333203/linear-regression-with-a-known-fixed-intercept-in-r

linear_reg() %>% 
  set_engine("stan")%>%
  fit(I(NPS-1.0) ~ 0+After+During*Age, data = df%>%select(-c(WTP,NPS_Fact)))%>%
  tidy(conf.int = TRUE)



```


```{r, eval=FALSE}
### Results for generalized model
linear_reg(penalty = double(1), mixture = double(1)) %>% 
  set_engine("glmnet")%>%
  fit(NPS ~ After+During*Age, data = df%>%select(-c(WTP,NPS_Fact)))%>%
  tidy()
```


\newpage

## Analysis of the comments
```{r}
myChampions_filtered <- myChampions%>%
  mutate(NPS = as.integer(myChampions$`Très bien. Maintenant faites l'hypothèse que cette solution puisse être offerte à 100CHF/test. Quelle est la probabilité que vous la recommandiez à un ami(e) ou à un de vos proches ?`)) %>%
  drop_na(NPS,`Expliquez votre choix :`)

covid_corpus <- corpus(
  myChampions_filtered$`Expliquez votre choix :`,
  docvars = data.frame(
    text=  myChampions_filtered$`Expliquez votre choix :`,
    scores = myChampions_filtered$NPS
  )
)

# processed <- textProcessor(covid_corpus$text, 
#                       metadata = covid_corpus
#                     )
# 
#       out02 <- prepDocuments(processed$documents, processed$vocab, processed$meta)

myDFM <- covid_corpus%>%
  dfm(remove = stopwords("french"), remove_punct = TRUE, stem = TRUE) %>%
  dfm_trim(min_termfreq = 2)%>%
  dfm_select(min_nchar = 3) # INCLUDE PCR

dfm2stm <- convert(myDFM, to = "stm")

colnames(dfm2stm$meta)[2] <- "scores"
```


```{r}
# findCovidK <- searchK(
#                  dfm2stm$documents, 
#                  dfm2stm$vocab,
#                  K = c(2:20),
#                  prevalence =~ as.integer(dfm2stm$meta$scores),
#                  data = dfm2stm$meta,
#                  verbose=FALSE
#                  )
# 
# save(findCovidK, file = "findCovidK.rda")

load("findCovidK.Rda") # Load file to save time

infoK <- data.frame(
chosenK  = reshape2::melt(findCovidK$results$K)$value,
myExclus = reshape2::melt(findCovidK$results$exclus)$value,  
mySemCoh = reshape2::melt(findCovidK$results$semcoh)$value
)

ggplot(infoK, aes(x= myExclus, y= mySemCoh)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  geom_tile(aes(fill=chosenK))+
  # facet_wrap(~Metric, scales = "free_y") +
  labs(x = "Exclusivity",
       y = "Semantic Coherence",
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 8")
  
plot(findCovidK) # Chosen value = 9

```


```{r}
model.stm <- stm(dfm2stm$documents, 
                 dfm2stm$vocab,
                 prevalence =~ as.integer(dfm2stm$meta$scores),
                 K = 9, 
                 data = dfm2stm$meta, 
                 init.type = "Spectral",
                 verbose = FALSE
               ) 

model.stm%>%plot()

```

```{r}
### Make semantic network
  
myFCM <- myDFM%>%
  fcm()

# myFCM%>%textplot_network(tryCatch({min_freq = input$myCorr_topics},error=function(e){min_freq = .95})) # TryCatch allows test offline  

```

### Semantic network to indetify relevant topics

```{r}
myTopFeatures <- myFCM%>%
  topfeatures(n=30,decreasing = TRUE
              # , scheme = "count"
              , scheme = "docfreq"
              # , groups = "author"
              )%>%names()

# Create dataframe with topics from STM
# https://juliasilge.com/blog/sherlock-holmes-stm/
myChosenTopics <- model.stm%>%
      tidy()%>%
        group_by(topic) %>%
        top_n(3, beta) %>%
        arrange(topic,beta)%>%
        ungroup()

### Add words from topics in the list
feat <- c(
  myChosenTopics$term, # Select only the column TERMS to have a character vector 
  myTopFeatures # Add the list obtained with Quanteda
  )

### Filter networks from list of relevant words
# https://quanteda.io/reference/textplot_network.html
myNiche <- myFCM%>%
    fcm_select(pattern = feat) 

### Define colors
# https://r-charts.com/colors/
myColorsList <- c("blue","brown1","burlywood4","cadetblue4",
              "#458B00","#8B4513","#EE6A50","#FF1493",
              "#B22222","darkgoldenrod2","darkolivegreen3","#EE7600",
              "#68228B","#C1FFC1","#2F4F4F","#FF1493")

### The FCM Matrix is made in an order that does not respect the order of the topics
## Step 1: We start by creating a dataframe with colors and size for each word

myNicheInfoLong <- data.frame(
      term=rownames(myNiche)
      # , myColor=rep("black",dim(myNiche)[1])
      # , myLabelSize=rep(3,dim(myNiche)[1])
  )%>%
  left_join(myChosenTopics, by=c("term"="term"))%>% #Left join with topicslist (This doubles some items in multiple topics)
  group_by(term)%>%
  dplyr::summarize(topic=max(topic),mean(beta),n=n())%>%
  mutate(myColor = ifelse(is.na(topic), "black", myColorsList[topic]))%>%
  mutate(myLabelSize = ifelse(is.na(topic), 3, 7))

## Step 2: Since summarize changes the order of the list, do again a left join
myNicheInfo <- data.frame(
      term=rownames(myNiche)
  )%>%
  left_join(myNicheInfoLong, by=c("term"="term"))  

### Create smeantic network with colors
set.seed(12) # Seed to get the same results every time
myNiche %>%
    textplot_network(min_freq = 0.95
                   , vertex_labelcolor = myNicheInfo$myColor
                   # , vertex_labelcolor = c(rep('gray40',10),rep('blue', 20))
                   , vertex_labelsize = myNicheInfo$myLabelSize
                     # , vertex_labelsize = 0.1*rowSums(myNiche)/min(rowSums(myNiche)))
                     )
```

```{r}
df_DFM <- cbind(
          NPS= myChampions_filtered$NPS
          , Before = factor(myChampions_filtered$`Etape 1 : Rendez-vous`)
          , During = factor(myChampions_filtered$`Etape 2 : Test`)
          , After = factor(myChampions_filtered$`Etape 3 : Résultat`)
          , Age = factor(myChampions_filtered$`Quel est votre âge`)
          , Vaccine = factor(myChampions_filtered$`Etes-vous vaccinés?`)
        , convert(myDFM,to="data.frame")%>%
          select(
            moin, trop, cher, util, antigéniqu,
            prix, plus, nouveau, justifi,
            pcr, solut,
            variant,
            risqu,
            test
                 )
  )
```


### Linear regression with relevant terms

```{r}

# myLM_DFM <- linear_reg() %>% 
#   set_engine("lm")%>%
#   # set_engine("lm")%>% 
#   fit(I(NPS-1.0) ~ 0 +TestResults+Age*TestExperience + nouveau + trop , 
#       data = df_DFM%>%
#             mutate(TestResults = relevel(After, ref = "Équivalent aux tests que j'utilise actuellement"))%>%
#             mutate(TestExperience = relevel(During, ref = "Équivalent aux tests que j'utilise actuellement"))%>%
#             mutate(Age = relevel(Age, ref = "<20 ans"))
#   )


df_DFM_02 <- df_DFM%>%
            mutate(Before = relevel(Before, ref = "Équivalent aux tests que j'utilise actuellement"), .keep="unused")%>%
            mutate(During = relevel(During, ref = "Équivalent aux tests que j'utilise actuellement", .keep="unused"))%>%
            mutate(After = relevel(After, ref = "Équivalent aux tests que j'utilise actuellement"), .keep="unused")%>%
            mutate(Age = relevel(Age, ref = "<20 ans", .keep="unused"))%>%
            select(NPS,Before,During,After,Age,Vaccine,nouveau,trop,pcr,test) # The dependent variable should be first to allow broom::Augment function

myLM_DFM <- lm(NPS ~ 0 +After+Age*During + nouveau + trop , 
      data = df_DFM_02
      )

myLM_DFM%>%tidy()%>%knitr::kable()%>%kable_styling("striped")


```
The new linear regression analysis confirms that respondents _aged >40_ have the tendency to give at least 7/10 for the NPS (p<0.01).  
If the _Test result_ phase is perceived as better than the current one, the scores goes up 3 points (p<0.05).
An loss in quality of _Testing phase_ leads to 1 point less, whereas an improvement in the _Testing phase_ in itself seems to be more problematic to understand: if respondents have less somewhere between 21 and 30 years, there is not effect (-9.5 + 9.35).
Instead, if they are aged between 41 and 50 years old, the score might be going down 7 points (p>0.40).

Although the model has many variables its explanatory power is fairly good: the Adjusted R2 of the model is `r round_any((myLM_DFM%>%glance())$adj.r.squared,.01)`.



```{r}
### DOTWHISKER to explain linear regression

# https://cran.r-project.org/web/packages/dotwhisker/vignettes/dotwhisker-vignette.html
  dwplot(list(
          lm(NPS ~ 0+After+During*Age , data = df_DFM_02),
          lm(NPS ~ 0+Vaccine+Before+After+During*Age , data = df_DFM_02),
          lm(NPS ~ 0 +After+Age*During + nouveau + trop , data = df_DFM_02)
          ),
         # dot_args = list(size = 2, color = "black"),
         # whisker_args = list(color = "black"),
         vline = geom_vline(xintercept = 0, colour = "grey50", linetype = 2),
         vars_order = c("Before", "During", "After", "Age", "Vaccine"),
         model_order = c("M1: Simple", "M2: Complete", "M3: Mixed")
         )+
    theme_bw(base_size = 11) + 
    # Setting `base_size` for fit the theme
    # No need to set `base_size` in most usage
    xlab("Coefficient Estimate") + ylab("") +
    geom_vline(xintercept = 0,
               colour = "grey60",
               linetype = 2) +
    ggtitle("Predicting NPS") +
    theme(
        plot.title = element_text(face = "bold"),
        legend.position = c(0.007, 0.01),
        legend.justification = c(0, 0),
        legend.background = element_rect(colour = "grey80"),
        legend.title = element_blank()
    ) 
```


\newpage

## Analysis of the job to be done
```{r, eval=FALSE}


covid_corpus <- corpus(
  myChampions$`Expliquez votre choix :`,
  docvars = data.frame(
    text=  myChampions$`Expliquez votre choix :`,
    scores <-myChampions$`Très bien. Maintenant faites l'hypothèse que cette solution puisse être offerte à 100CHF/test. Quelle est la probabilité que vous la recommandiez à un ami(e) ou à un de vos proches ?`
  )
  )

# processed <- textProcessor(covid_corpus$text, 
#                       metadata = covid_corpus
#                     )
# 
#       out02 <- prepDocuments(processed$documents, processed$vocab, processed$meta)

myDFM <- covid_corpus%>%
  dfm(remove = stopwords("french"), remove_punct = TRUE, stem = TRUE) %>%
  dfm_trim(min_termfreq = 2)%>%
  dfm_select(min_nchar = 3) # INCLUDE PCR

dfm2stm <- convert(myDFM, to = "stm")

colnames(dfm2stm$meta)[2] <- "scores"
```


```{r, eval=FALSE}
# findCovidK <- searchK(
#                  dfm2stm$documents, 
#                  dfm2stm$vocab,
#                  K = c(2:20),
#                  prevalence =~ as.integer(dfm2stm$meta$scores),
#                  data = dfm2stm$meta,
#                  verbose=FALSE
#                  )
# 
# save(findCovidK, file = "findCovidK.rda")

load("findCovidK.Rda") # Load file to save time

infoK <- data.frame(
chosenK  = reshape2::melt(findCovidK$results$K)$value,
myExclus = reshape2::melt(findCovidK$results$exclus)$value,  
mySemCoh = reshape2::melt(findCovidK$results$semcoh)$value
)

ggplot(infoK, aes(x= myExclus, y= mySemCoh)) +
  geom_line(size = 1.5, alpha = 0.7, show.legend = FALSE) +
  geom_tile(aes(fill=chosenK))+
  # facet_wrap(~Metric, scales = "free_y") +
  labs(x = "Exclusivity",
       y = "Semantic Coherence",
       title = "Model diagnostics by number of topics",
       subtitle = "These diagnostics indicate that a good number of topics would be around 8")
  
plot(findCovidK) # Chosen value = 9

```


```{r, eval=FALSE}
model.stm <- stm(dfm2stm$documents, 
                 dfm2stm$vocab,
                 prevalence =~ as.integer(dfm2stm$meta$scores),
                 K = 9, 
                 data = dfm2stm$meta, 
                 init.type = "Spectral",
                 verbose = FALSE
               ) 

model.stm%>%plot()

```

```{r, eval=FALSE}
### Make semantic network
  
myFCM <- myDFM%>%
  fcm()

# myFCM%>%textplot_network(tryCatch({min_freq = input$myCorr_topics},error=function(e){min_freq = .95})) # TryCatch allows test offline  

```

### Semantic network to indetify relevant job to be done

```{r, eval=FALSE}
myTopFeatures <- myFCM%>%
  topfeatures(n=30,decreasing = TRUE
              # , scheme = "count"
              , scheme = "docfreq"
              # , groups = "author"
              )%>%names()

# Create dataframe with topics from STM
# https://juliasilge.com/blog/sherlock-holmes-stm/
myChosenTopics <- model.stm%>%
      tidy()%>%
        group_by(topic) %>%
        top_n(3, beta) %>%
        arrange(topic,beta)%>%
        ungroup()

### Add words from topics in the list
feat <- c(
  myChosenTopics$term, # Select only the column TERMS to have a character vector 
  myTopFeatures # Add the list obtained with Quanteda
  )

### Filter networks from list of relevant words
# https://quanteda.io/reference/textplot_network.html
myNiche <- myFCM%>%
    fcm_select(pattern = feat) 

### Define colors
# https://r-charts.com/colors/
myColorsList <- c("blue","brown1","burlywood4","cadetblue4",
              "#458B00","#8B4513","#EE6A50","#FF1493",
              "#B22222","darkgoldenrod2","darkolivegreen3","#EE7600",
              "#68228B","#C1FFC1","#2F4F4F","#FF1493")

### The FCM Matrix is made in an order that does not respect the order of the topics
## Step 1: We start by creating a dataframe with colors and size for each word

myNicheInfoLong <- data.frame(
      term=rownames(myNiche)
      # , myColor=rep("black",dim(myNiche)[1])
      # , myLabelSize=rep(3,dim(myNiche)[1])
  )%>%
  left_join(myChosenTopics, by=c("term"="term"))%>% #Left join with topicslist (This doubles some items in multiple topics)
  group_by(term)%>%
  dplyr::summarize(topic=max(topic),mean(beta),n=n())%>%
  mutate(myColor = ifelse(is.na(topic), "black", myColorsList[topic]))%>%
  mutate(myLabelSize = ifelse(is.na(topic), 3, 7))

## Step 2: Since summarize changes the order of the list, do again a left join
myNicheInfo <- data.frame(
      term=rownames(myNiche)
  )%>%
  left_join(myNicheInfoLong, by=c("term"="term"))  

### Create smeantic network with colors
set.seed(12) # Seed to get the same results every time
myNiche %>%
    textplot_network(min_freq = 0.95
                   , vertex_labelcolor = myNicheInfo$myColor
                   # , vertex_labelcolor = c(rep('gray40',10),rep('blue', 20))
                   , vertex_labelsize = myNicheInfo$myLabelSize
                     # , vertex_labelsize = 0.1*rowSums(myNiche)/min(rowSums(myNiche)))
                     )
```

```{r, eval=FALSE}
df_DFM_jtbd <- cbind(
          # ID=myChampions$ID
          NPS=as.integer(
            myChampions$`Très bien. Maintenant faites l'hypothèse que cette solution puisse être offerte à 100CHF/test. Quelle est la probabilité que vous la recommandiez à un ami(e) ou à un de vos proches ?`
          )
          , Before = factor(myChampions$`Etape 1 : Rendez-vous`)
          , During = factor(myChampions$`Etape 2 : Test`)
          , After = factor(myChampions$`Etape 3 : Résultat`)
          , Age = factor(myChampions$`Quel est votre âge`)
        , convert(myDFM,to="data.frame")%>%
          select(
            moin, trop, cher, util, antigéniqu,
            prix, plus, nouveau, justifi,
            pcr, solut,
            variant,
            risqu,
            test
                 )
  )
```




\newpage

## Appendix: Collected data
```{r}
# It doesn't look well on MS Word ...
df%>% knitr::kable()%>%kableExtra::kable_styling("striped")

```

\newpage


## Appendix: Prediction on the overall dataset
```{r}
# It doesn't look well on MS Word ...

myLM_DFM_aug <- augment(myLM_DFM)

cbind(myLM_DFM_aug%>%select(During,After,Age,nouveau,trop,
         NPS),
         fitted=round_any(myLM_DFM_aug$.fitted,.01),
         resid=round_any(myLM_DFM_aug$.resid,.01)
        )%>%
  arrange(desc(resid))%>%
  knitr::kable()%>%kableExtra::kable_styling("striped")

```

\newpage



## Appendix: Testing the linear regression model 


```{r}
myIterations <- 100
rsq2_avg <- rep(NA,myIterations)
myProp <- 0.7
```

Results from training the model with `r 100*myProp`% of the data and testing it with the remaining data.  
We perform `r myIterations` iterations with different seeds, and we show here the results.  

```{r}
for(i in 1:myIterations){

set.seed(i)
rf_test_pred_2 <- NA

df_split <- initial_split(
  # rbind(df_DFM_02,df_DFM_02,df_DFM_02)# Data Augmentation
  df_DFM_02
  , prop = myProp
  # , strata=After
  )
df_train <- training(df_split)
df_test <- testing(df_split)

# Model fit with Tidymodels
model_fit <- linear_reg() %>% 
  set_engine("lm")%>%
  fit(NPS ~ After+Age*During + nouveau + trop, data=df_train)

# Prediction
tryCatch(
  {
  rf_test_pred_2 <- 
  predict(model_fit, df_test) %>% 
  bind_cols(df_test %>% select(NPS))   # Add the true outcome data back in
  }, error=function(e){}
  )

# ggplot(data = rf_test_pred_2,
#        mapping = aes(x = .pred, y = NPS)) +
#   geom_point(color = '#006EA1', alpha = 0.25) +
#   geom_abline(intercept = 0, slope = 1, color = 'orange') +
#   labs(title = paste('NPS for 100 CHF (R2=',round_any(rsq(rf_test_pred_2, NPS, .pred)$.estimate,.01),')'),
#        x = 'Predicted NPS',
#        y = 'Actual NPS')

tryCatch(
  {
rsq2_avg[i] <- rsq(rf_test_pred_2, NPS, .pred)$.estimate
  }, error=function(e){}
  )


}

myR2 <- data.frame(R2=round_any(rsq2_avg,.05))%>%group_by(R2)%>%dplyr::summarise(Freq=n())

library(ggplot2)
ggplot(myR2, aes(R2, Freq)) +     
  geom_col(position = 'dodge')

myNA <- sum(is.na(rsq2_avg))
myMedian <- round_any(median(rsq2_avg, na.rm = TRUE ),.01)

```

After `r myIterations` iterations and *`r myNA` missing values*, the median value for the R2 is `r myMedian`. 

### Data augmentation to reduce the number of missing R2.

Missing values are linked to the small sample. It is possible that the `r 100*myProp`% in the training sample do not have all the possible values that the testing sample has, and this generates an error.   
A possible solution is to duplicate the dataset and have `r dim(df_DFM_02)[1]` *2= `r dim(rbind(df_DFM_02,df_DFM_02))[1]` lines.

```{r}
for(i in 1:myIterations){

set.seed(i)
rf_test_pred_2 <- NA

df_split <- initial_split(
  # rbind(df_DFM_02,df_DFM_02,df_DFM_02)# Data Augmentation
  rbind(df_DFM_02,df_DFM_02)# Data Augmentation
  # df_DFM_02
  , prop = myProp
  # , strata=After
  )
df_train <- training(df_split)
df_test <- testing(df_split)

# Model fit with Tidymodels
model_fit <- linear_reg() %>% 
  set_engine("lm")%>%
  fit(NPS ~ After+Age*During + nouveau + trop, data=df_train)

# Prediction
tryCatch(
  {
  rf_test_pred_2 <- 
  predict(model_fit, df_test) %>% 
  bind_cols(df_test %>% select(NPS))   # Add the true outcome data back in
  }, error=function(e){}
  )

# ggplot(data = rf_test_pred_2,
#        mapping = aes(x = .pred, y = NPS)) +
#   geom_point(color = '#006EA1', alpha = 0.25) +
#   geom_abline(intercept = 0, slope = 1, color = 'orange') +
#   labs(title = paste('NPS for 100 CHF (R2=',round_any(rsq(rf_test_pred_2, NPS, .pred)$.estimate,.01),')'),
#        x = 'Predicted NPS',
#        y = 'Actual NPS')

tryCatch(
  {
rsq2_avg[i] <- rsq(rf_test_pred_2, NPS, .pred)$.estimate
  }, error=function(e){}
  )


}

myR2 <- data.frame(R2=round_any(rsq2_avg,.05))%>%group_by(R2)%>%dplyr::summarise(Freq=n())


ggplot(myR2, aes(R2, Freq)) +     
  geom_col(position = 'dodge')

myNA_02 <- sum(is.na(rsq2_avg))
myMedian_02 <- round_any(median(rsq2_avg, na.rm = TRUE ),.01)

```

After `r myIterations` iterations and `r myNA_02` missing values, the median value for the R2 is `r myMedian_02`. 

\newpage

## Appendix: Exploratory Data Analysis with Random Forest

In this section, the systems analyzes 70% of the collected data (=0.7 * `r dim(answers_raw)[1]`) and test its model to predict 10 answers.
The results are shown below.

```{r}

set.seed(1)
df_split <- initial_split(df, prop = 0.7)
df_train <- training(df_split)
df_test <- testing(df_split)

# Tidymodels
df_recipe <- recipe(NPS_Fact ~ . , data = 
                    # df_train[,6:9] # Use only demographic data
                    df_train%>%select(-c(NPS,WTP))
)

#building model
df_model <- rand_forest() %>% 
  set_engine("randomForest") %>% 
  set_mode("classification")

#workflow
wf <- workflow() %>%
   add_recipe(df_recipe) %>%
   add_model(df_model) 

# Model fit
model_fit <- wf %>% 
    parsnip::fit(df)

# Prediction
rf_test_pred_2 <- 
  predict(model_fit, df_test) %>% 
  bind_cols(predict(model_fit, df_test, type = "prob")) %>% 
  # Add the true outcome data back in
  bind_cols(df_test %>% select(NPS_Fact))

# myRocAuc <- rf_test_pred_2 %>%                # training set predictions
#   roc_auc(truth = NPS_Fact, .pred_class)

myAccuracy <- rf_test_pred_2 %>%                # training set predictions
  accuracy(truth = NPS_Fact, .pred_class)

rf_test_pred_2%>%knitr::kable()%>%kable_styling("striped")
# The accuracy of the Random forest model is `r myAccuracy$.estimate` (1.0 = 100% being the maximum).

```


```{r, include=FALSE}
library(inTrees)
# https://stackoverflow.com/questions/14996619/random-forest-output-interpretation


library(randomForest)

X <- df_train%>%select(-c(NPS,WTP))
target <- df_train$NPS_Fact

treeList <- RF2List(
  randomForest(
              NPS_Fact ~ . , data = 
                    # rbind(df_train[,6:9],df_train[,6:9]) # Use only demographic data
                    df_train%>%select(-c(NPS,WTP))
              , localImp = TRUE
            )
  )  # transform rf object to an inTrees' format
exec <- extractRules(treeList, 
                     df_train
                     # rbind(df_train[,6:9],df_train[,6:9])
                     )  # R-executable conditions
# exec[1:2,]

# get rule metrics
ruleMetric <- getRuleMetric(exec,X,target)  
# ruleMetric[1:2,]

(ruleMetric <- selectRuleRRF(ruleMetric, X, target))

(learner <- buildLearner(ruleMetric, X, target))
```

The classification rules extracted by the system are ...

```{r }
presentRules(ruleMetric, colnames(X))%>%
      as.data.frame()%>%
    # filter(pred=="1")%>%
     distinct(len,freq,err,condition,pred)%>%
      arrange(desc(pred), err, desc(freq), desc(len))%>%knitr::kable()%>% kable_styling("striped")

```

\newpage

Appendix: 

```{r}
#Source: https://cran.r-project.org/web/packages/broom/vignettes/broom.html

myLM_base%>%tidy()

myLM_base%>%glance()
```
\newpage

## Appendix: Max WTP and WTP

```{r}
# colnames(answers_raw)[40] = "max_WTP"
# colnames(answers_raw)[41] = "real_WTP"

myCodes <- fixAccentsDataframe(
  read.xlsx("data/codes.xlsx", header = TRUE, sheetName = "Sheet1")
)


maxWTP <- answers_raw%>%
  mutate(max_WTP = `En faisant l'hypothèse que l'assurance ne paie pas, quel serait selon vous le prix CORRECT  pour cette nouvelle solution de test ?`)%>%
  left_join(myCodes, by=c("max_WTP"="Input"))%>%
  select("Output")

realWTP <- answers_raw%>%  
  mutate(real_WTP = `En faisant l'hypothèse que l'assurance ne paie pas, quel serait le prix MAXIMAL que vous seriez prêt(e) à payer pour cette nouvelle solution de test ?`)%>%
  left_join(myCodes, by=c("real_WTP"="Input"))%>%
  select("Output")

scores<- data.frame(
  x= answers_raw$ID,
  rbind(maxWTP,realWTP),
  group="Real"
  )

scores$group[1:dim(maxWTP)[1]] = "Max"

scores %>%
  ggplot( aes(x=x, y=Output, group=group, color=group)) +
    geom_line()

library(plyr)


meanReal <- round_any(mean(realWTP$Output),.5)
meanMax <- round_any(mean(maxWTP$Output),.5)
difMean <- abs(meanMax- meanReal)
```

-	La courbe bleue montre les prix corrects de la nouvelle solution qu’on partagé les sondés tandis que la courbe orange montre les prix maximaux qu’ils seraient prêts à débourser pour une telle solution
-	Nous remarquons que la grande majorité des sondés ont défini des prix maximaux qui se trouvent en dessous des prix qu’ils trouvent corrects pour une telle solution. 

-	La moyenne des prix corrects estimés par le panel est de : 	CHF `r meanReal`
-	La moyenne des prix maximaux estimés par le panel est de : 	CHF `r meanMax`

-	La différence moyenne entre la perception de ces deux prix est d’environ : 	CHF `r difMean`


\newpage